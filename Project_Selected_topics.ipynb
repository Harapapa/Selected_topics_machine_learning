{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "501a2a5d",
   "metadata": {},
   "source": [
    "# Energy Consumption Forecasting with CUDA and Machine Learning\n",
    "## Selected Topics in Machine Learning\n",
    "\n",
    "**Objective**: Predict household energy consumption using CUDA-accelerated computations and advanced machine learning techniques including semi-supervised learning.\n",
    "\n",
    "## Project Structure\n",
    "1. **Setup & Data Loading**\n",
    "2. **Exploratory Data Analysis (EDA)**\n",
    "3. **Data Preprocessing** \n",
    "4. **CUDA Activation** \n",
    "5. **Feature Engineering with CUDA** \n",
    "6. **CUDA based solution**\n",
    "7. **Semi-Supervised Learning (Label Spreading)**\n",
    "8. **Traditional ML Models**\n",
    "9. **Conclusion**\n",
    "\n",
    "### Dataset : UCI Individual Household Electric Power Consumption\\n\",\n",
    "- **Source**: https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption\\n\",\n",
    "- **Format**: TXT file with semicolon separator\\n\",\n",
    "- **Features**: Date, Time, Global_active_power, Global_reactive_power, Voltage, Global_intensity, Sub_metering (1, 2, 3)\\n\",\n",
    "- **Records**: ~2 million measurements (minute-level data from 2006-2010)\\n\",\n",
    "- **Target**: Global_active_power (household energy consumption in kW)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4454fbe",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading\n",
    "\n",
    "Import necessary libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1d8269",
   "metadata": {},
   "source": [
    "Load raw household power consumption dataset from text file.\n",
    "This cell reads the UCI Machine Learning Repository's household power consumption dataset using Pandas' CSV reader with semicolon delimiter. The dataset contains minute-level measurements of electrical quantities from a single household over 4 years. The `low_memory=False` parameter ensures consistent data type inference across all rows. After loading, it displays the first few rows to verify structure. This is the initial data ingestion step before any cleaning or transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = 'household_power_consumption.txt'\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(data_path, sep=';', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b060451b",
   "metadata": {},
   "source": [
    "## 2. Data Exploration & Visualization\n",
    "\n",
    "Let's understand our dataset and visualize the patterns in energy consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c2f0a",
   "metadata": {},
   "source": [
    "Display DataFrame metadata including column types and memory usage.\n",
    "This cell invokes Pandas' `.info()` method to provide a comprehensive summary of the dataset structure. It shows the number of entries, column names, non-null counts, and data types for each feature. This diagnostic view helps identify missing values, data type inconsistencies, and memory footprint. The information is crucial for planning preprocessing steps. It's a standard exploratory data analysis (EDA) technique to understand dataset characteristics before manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659c029",
   "metadata": {},
   "source": [
    "Return dataset dimensions (rows × columns).\n",
    "This cell uses the `.shape` attribute to display the total number of observations and features in the raw dataset. The output is a tuple where the first value represents rows (time observations) and the second represents columns (measured variables). This quick check confirms the dataset size matches expectations from documentation. Understanding dimensionality guides computational resource planning. It's the simplest EDA metric to assess data volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c4c04f",
   "metadata": {},
   "source": [
    "Generate descriptive statistics for numerical columns.\n",
    "This cell computes summary statistics (count, mean, std, min, quartiles, max) for all numeric features using Pandas' `.describe()` method. The statistics reveal central tendency, dispersion, and range of each variable, helping identify outliers and distribution characteristics. This statistical overview guides feature scaling decisions and anomaly detection. The percentiles (25%, 50%, 75%) show data spread across the distribution. It's a fundamental EDA step to understand numerical variable behavior before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cca74e",
   "metadata": {},
   "source": [
    "Display dataset temporal coverage and missing value counts.\n",
    "This cell prints the date range of observations by accessing the first and last entries of the Date column, establishing the dataset's time span. It then uses `.isnull().sum()` to count missing values per column, identifying data quality issues. Missing data can appear as '?' strings or NaN values in this dataset. Understanding temporal boundaries helps contextualize consumption patterns (seasonal effects, long-term trends). This diagnostic informs the imputation strategy needed in preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ee578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic informations about the dataset\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(f\"\\n Date Range:\")\n",
    "print(f\"   From: {df['Date'].iloc[0]}\")\n",
    "print(f\"   To:   {df['Date'].iloc[-1]}\")\n",
    "print(f\"\\n Missing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29acbd72",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Clean the data and prepare it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb4046",
   "metadata": {},
   "source": [
    "Transform and clean raw data into analysis-ready format.\n",
    "This cell performs critical preprocessing: creating a unified datetime index by combining Date and Time columns, converting '?' placeholders to NaN, and coercing all columns to numeric types. It applies forward-fill then backward-fill imputation to handle missing values, preserving temporal continuity. The data is resampled from minute-level to hourly averages using `.resample('H').mean()`, reducing noise and computational load. These transformations establish a clean, uniformly-spaced time series suitable for machine learning. The final shape confirms successful dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e238d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Date and Time into a single datetime column\n",
    "# Create datetime format.\n",
    "df['datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "df.set_index('datetime', inplace=True)\n",
    "df.drop(['Date', 'Time'], axis=1, inplace=True) #use the formatted datetime and drop the two basic\n",
    "print(\"Datetime index created\")\n",
    "\n",
    "# Handle missing values.\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "print(f\" Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Resample to hourly data (reduces size and noise)\n",
    "df_hourly = df.resample('H').mean()\n",
    "print(\"\\n✓ Preprocessing complete!\")\n",
    "print(f\"\\n Final dataset after resampling: {df_hourly.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0406b1",
   "metadata": {},
   "source": [
    "Preview cleaned hourly dataset structure.\n",
    "This cell displays the first 5 rows of the preprocessed hourly-resampled DataFrame using `.head()`. It allows visual verification that datetime indexing, numeric conversion, and aggregation were successful. Inspecting these rows confirms that average values per hour look reasonable and no obvious data corruption occurred. This quick sanity check ensures the preprocessing pipeline produced valid output before proceeding to analysis. The tabular view shows all features alongside their datetime index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b73c77",
   "metadata": {},
   "source": [
    "Display hourly DataFrame metadata after preprocessing.\n",
    "This cell reruns `.info()` on the cleaned hourly dataset to show the transformed schema. It confirms the datetime index replaced the original Date/Time columns and verifies the new row count matches hourly resampling expectations. The output shows updated non-null counts reflecting the imputation process. Comparing this info to the raw dataset reveals the impact of preprocessing transformations. This validation step ensures data integrity before feature engineering and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8232cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277358b8",
   "metadata": {},
   "source": [
    "Visualize time series patterns at multiple temporal scales.\n",
    "This cell creates a two-panel matplotlib figure showing energy consumption trends: the complete 4-year time series and a detailed one-week sample. The top plot reveals long-term patterns, seasonality, and any data gaps using a semi-transparent line. The bottom plot zooms into hourly variation with markers, exposing daily cycles and short-term fluctuations. Statistical summaries (mean, peak, minimum) are printed to quantify typical and extreme consumption levels. These visualizations establish baseline understanding of temporal dynamics before extracting engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cleaned data\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "\n",
    "# Plot 1: Full time series\n",
    "axes[0].plot(df_hourly.index, df_hourly['Global_active_power'], linewidth=0.5, alpha=0.7)\n",
    "axes[0].set_title('Household Energy Consumption Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Power (kW)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: One week sample\n",
    "one_week = df_hourly['2007-01-01':'2007-01-07']\n",
    "axes[1].plot(one_week.index, one_week['Global_active_power'], linewidth=2, marker='o', markersize=3)\n",
    "axes[1].set_title('One Week Sample (Jan 1-7, 2007)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Power (kW)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average consumption: {df_hourly['Global_active_power'].mean():.2f} kW\")\n",
    "print(f\"Peak consumption: {df_hourly['Global_active_power'].max():.2f} kW\")\n",
    "print(f\"Minimum consumption: {df_hourly['Global_active_power'].min():.2f} kW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9946fce8",
   "metadata": {},
   "source": [
    "### Pattern Analysis\n",
    "\n",
    "Let's explore how energy consumption varies by hour, day, and month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27927ff",
   "metadata": {},
   "source": [
    "Analyze and visualize consumption patterns across temporal dimensions.\n",
    "This cell extracts temporal features (hour, day_of_week, month, season) from the datetime index and creates a 2×2 grid of bar charts showing average consumption by each dimension. It uses Pandas' `.groupby()` aggregation to compute mean power for each time unit, revealing daily cycles, weekly patterns, monthly trends, and seasonal variations. The visualizations expose peak consumption periods and low-demand times. Statistical summaries identify the specific hour, day, and season with highest usage. This exploratory analysis informs feature engineering choices for capturing cyclical patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c857c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time features for pattern analysis\n",
    "df_hourly['hour'] = df_hourly.index.hour\n",
    "df_hourly['day_of_week'] = df_hourly.index.dayofweek\n",
    "df_hourly['month'] = df_hourly.index.month\n",
    "df_hourly['season'] = df_hourly.index.month % 12 // 3 + 1  # 1=Winter, 2=Spring, 3=Summer, 4=Fall\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Hourly pattern\n",
    "hourly_avg = df_hourly.groupby('hour')['Global_active_power'].mean() #órák szerinti átlagolás\n",
    "axes[0, 0].bar(hourly_avg.index, hourly_avg.values, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "axes[0, 0].set_title(' Average Consumption by Hour of Day', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Hour')\n",
    "axes[0, 0].set_ylabel('Power (kW)')\n",
    "axes[0, 0].set_xticks(range(0, 24, 2)) #0 től megyünk 24 óráig 2 órás lépésközzel\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y') \n",
    "\n",
    "# Day of week pattern\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "daily_avg = df_hourly.groupby('day_of_week')['Global_active_power'].mean() #Átlagot számolunk \n",
    "axes[0, 1].bar(range(7), daily_avg.values, color='coral', alpha=0.8, edgecolor='black') #range 7 = days of week\n",
    "axes[0, 1].set_title(' Average Consumption by Day of Week', fontsize=12, fontweight='bold') \n",
    "axes[0, 1].set_xlabel('Day')\n",
    "axes[0, 1].set_ylabel('Power (kW)')\n",
    "axes[0, 1].set_xticks(range(7)) # 7 nap van a héten\n",
    "axes[0, 1].set_xticklabels(day_names)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Monthly pattern\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "monthly_avg = df_hourly.groupby('month')['Global_active_power'].mean() #Hónapok szerinti átlagolás\n",
    "axes[1, 0].bar(monthly_avg.index, monthly_avg.values, color='seagreen', alpha=0.8, edgecolor='black')\n",
    "axes[1, 0].set_title(' Average Consumption by Month', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "axes[1, 0].set_ylabel('Power (kW)')\n",
    "axes[1, 0].set_xticks(range(1, 13)) #1-12 hónap\n",
    "axes[1, 0].set_xticklabels(month_names, rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Seasonal pattern\n",
    "season_names = ['Winter', 'Spring', 'Summer', 'Fall'] #4 évszak van\n",
    "seasonal_avg = df_hourly.groupby('season')['Global_active_power'].mean() # évszakok szerint átlagolunk\n",
    "colors = ['lightblue', 'lightgreen', 'gold', 'orange']\n",
    "axes[1, 1].bar(range(1, 5), seasonal_avg.values, color=colors, edgecolor='black', alpha=0.8) #4 évszak feltüntetése \n",
    "axes[1, 1].set_title(' Average Consumption by Season', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Season')\n",
    "axes[1, 1].set_ylabel('Power (kW)')\n",
    "axes[1, 1].set_xticks(range(1, 5)) #4 évszak x tengely szerinti pozicionálása\n",
    "axes[1, 1].set_xticklabels(season_names)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Key Observations:\")\n",
    "print(f\"   Peak hour: {hourly_avg.idxmax()}:00 ({hourly_avg.max():.2f} kW)\")\n",
    "print(f\"   Lowest hour: {hourly_avg.idxmin()}:00 ({hourly_avg.min():.2f} kW)\")\n",
    "print(f\"   Highest consumption season: {season_names[seasonal_avg.idxmax()-1]} ({seasonal_avg.max():.2f} kW)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ad1b9",
   "metadata": {},
   "source": [
    "## 3. CUDA Programming \n",
    "\n",
    "Detect and configure CUDA GPU availability for parallel computation.\n",
    "This cell imports Numba's CUDA module and checks if a compatible NVIDIA GPU is accessible using `cuda.is_available()`. If detected, it retrieves and prints the GPU device name (e.g., GTX 950M) to confirm hardware identity. If unavailable, it notifies that CPU fallback will be used for computations. This runtime check determines whether subsequent CUDA kernels can leverage GPU parallelism or must use NumPy alternatives. The setup establishes the computational backend for accelerated feature engineering operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CUDA libraries\n",
    "from numba import cuda\n",
    "\n",
    "print(\"CUDA SETUP CHECK\")\n",
    "\n",
    "if cuda.is_available():\n",
    "    print(\" CUDA is available!\")\n",
    "    gpu = cuda.get_current_device()\n",
    "    print(f\"   GPU: {gpu.name.decode()}\")\n",
    "else:\n",
    "    print(\" CUDA not available - will use CPU fallback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5056be",
   "metadata": {},
   "source": [
    "### 3.1 CUDA Rolling Statistics Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4754c11",
   "metadata": {},
   "source": [
    "Implement GPU-accelerated rolling mean calculation using CUDA kernel.\n",
    "This cell defines a CUDA kernel `roll_mean` decorated with `@cuda.jit` that computes sliding window averages in parallel across GPU threads. Each thread calculates the mean of a window ending at position `i`, handling edge cases where the window extends before index 0. The wrapper function `gpu_roll_mean` manages memory transfer (CPU→GPU), launches the kernel with appropriate block/grid dimensions (256 threads per block), synchronizes execution, and copies results back to host memory. This parallel implementation significantly accelerates rolling statistics computation compared to sequential CPU loops, essential for large time series datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def roll_mean(x, out, w):\n",
    "    \"\"\"\n",
    "    x   : Input array (power consumption values) - stored in GPU memory\n",
    "    out : Output array (rolling means) - stored in GPU memory  \n",
    "    w   : Window size (integer) - how many past values to average\n",
    "    \"\"\"\n",
    "    #Give each thread a unique index from 0 to array_size-1 and computes blockIdx.x * blockDim.x + threadIdx.x\n",
    "    i = cuda.grid(1)\n",
    "    if i < out.size:\n",
    "        start = i - w + 1 # if window size is 24 at position i=30 then it starts from index 7.\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        sw = 0.0 # Sum of values in window\n",
    "        count = 0 #Count of values in window\n",
    "        for j in range(start, i + 1):\n",
    "            sw += x[j]\n",
    "            count += 1\n",
    "        out[i] = sw / count\n",
    "        # Demonstration:\n",
    "        # x contains hourly power readings: [1.2, 0.8, 0.9, ..., 2.5, 3.1, 2.8] kW\n",
    "        # At position i=25 (second day, hour 1) with w=24:\n",
    "        #   start = 25 - 24 + 1 = 2\n",
    "        #   Loop iterates j=2 to j=25 (24 hours)\n",
    "        #   Accumulates: s = 0.9 + 1.1 + ... + 2.5 + 3.1 = 42.3 kW (total)\n",
    "        #   Count: c = 24 hours\n",
    "        #   Result: out[25] = 42.3 / 24 = 1.76 kW (24-hour average power)\n",
    "\n",
    "def gpu_roll_mean(x, w):\n",
    "    x = x.astype(np.float64) # specific data type for CUDA compatibility which is float64\n",
    "    d_x = cuda.to_device(x) #Memory transfer to GPU, this is a blocking operation so the CPU waits until it's done\n",
    "    d_out = cuda.device_array(x.size, np.float64) #Creates empty array in GPU, values are uninitialized (random) until kernel writes to them\n",
    "    tpb = 256 #Threads Per Block \n",
    "    bpg = (x.size + tpb - 1) // tpb # Blocks Per Grid, bpg = (35000 + 255) / 256 = 137 blocks an example for how many blocks are needed\n",
    "    roll_mean[bpg, tpb](d_x, d_out, int(w)) #roll mean is called which will execute threads the roll_mean function with different i value\n",
    "    cuda.synchronize() #Ensures results are ready before we copy them back\n",
    "    return d_out.copy_to_host() #Copies result array from GPU memory back to CPU\n",
    "\n",
    "print(\"CUDA rolling mean ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c69f256",
   "metadata": {},
   "source": [
    "### 3.2 CUDA Swish Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e2a39",
   "metadata": {},
   "source": [
    "Implement GPU-accelerated Swish activation function using CUDA kernel.\n",
    "This cell defines a CUDA kernel that applies the Swish activation function (x / (1 + e^(-x))) element-wise across an array in parallel. The Swish function, popularized in deep learning, provides smooth non-linear transformation superior to ReLU for certain tasks. Each GPU thread computes Swish for one array element independently, enabling massive parallelism. The wrapper `cuda_swish` handles data type conversion to float32, memory transfers, kernel launch configuration, and result retrieval. This non-linear feature transform can capture hour-of-day patterns more expressively than raw hour values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "@cuda.jit\n",
    "def swish_kernel(x, out):\n",
    "    i = cuda.grid(1)\n",
    "    if i < x.size:\n",
    "        v = x[i]\n",
    "        out[i] = v / (1.0 + math.exp(-v))\n",
    "\n",
    "\n",
    "def cuda_swish(x):\n",
    "    x = x.astype(np.float32)\n",
    "    d_x = cuda.to_device(x)\n",
    "    d_out = cuda.device_array(x.size, dtype=np.float32)\n",
    "    tpb = 256\n",
    "    bpg = (x.size + tpb - 1) // tpb\n",
    "    swish_kernel[bpg, tpb](d_x, d_out)\n",
    "    cuda.synchronize()\n",
    "    return d_out.copy_to_host()\n",
    "\n",
    "print(\"✅ Minimal CUDA Swish ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c8b52",
   "metadata": {},
   "source": [
    "### 3.3 Apply CUDA Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6a7cac",
   "metadata": {},
   "source": [
    "Generate engineered features using CUDA kernels or CPU fallback.\n",
    "This cell creates four features: 24-hour rolling mean of power consumption (using GPU or CPU), Swish-transformed normalized hour values, and sine/cosine cyclical encodings of hour. It measures execution time to demonstrate GPU acceleration benefits. The rolling mean captures temporal momentum, Swish adds non-linearity to hour patterns, and sin/cos encodings preserve cyclical continuity (hour 23 is close to hour 0). These engineered features improve model capacity to capture daily cycles and temporal dependencies. All features are added as new columns to `df_hourly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simplified CUDA feature engineering\n",
    "print(\"CUDA FEATURE ENGINEERING\")\n",
    "start_time = time.time()\n",
    "power_data = df_hourly['Global_active_power'].values.astype(np.float64)\n",
    "\n",
    "if cuda.is_available():\n",
    "    rolling_mean_24h = gpu_roll_mean(power_data, 24)\n",
    "    df_hourly['power_rolling_mean_24h'] = rolling_mean_24h\n",
    "    hour_norm = (df_hourly['hour'].values - 12) / 12\n",
    "    df_hourly['hour_swish'] = cuda_swish(hour_norm.astype(np.float32))\n",
    "    mode = \"GPU\"\n",
    "else:\n",
    "    df_hourly['power_rolling_mean_24h'] = df_hourly['Global_active_power'].rolling(24).mean()\n",
    "    hour_norm = (df_hourly['hour'].values - 12) / 12\n",
    "    df_hourly['hour_swish'] = hour_norm * (1 / (1 + np.exp(-hour_norm)))\n",
    "    mode = \"CPU\"\n",
    "\n",
    "hours = df_hourly['hour'].values.astype(float)\n",
    "angle = 2 * np.pi * hours / 24.0\n",
    "df_hourly['hour_sin'] = np.sin(angle)\n",
    "df_hourly['hour_cos'] = np.cos(angle)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Features created ({mode}) in {elapsed:.3f}s: power_rolling_mean_24h, hour_swish, hour_sin, hour_cos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f60d05",
   "metadata": {},
   "source": [
    "Visualize engineered features to validate transformation quality.\n",
    "This cell creates a 3-panel matplotlib figure displaying the newly created features: rolling mean overlaid on actual power (top), Swish-transformed hour values (middle), and sine/cosine cyclical encodings (bottom). The rolling mean plot shows how the 24-hour window smooths consumption fluctuations. The Swish scatter reveals the non-linear mapping from hour to activation value. The cyclical encoding plot demonstrates how sine and cosine jointly represent hour as coordinates on a circle, eliminating boundary discontinuities. These visualizations confirm feature engineering succeeded and reveal their distributional properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64850809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of engineered features \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "week = df_hourly['2007-02-01':'2007-02-07']\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 9))\n",
    "\n",
    "# Rolling mean vs actual\n",
    "axes[0].plot(week.index, week['Global_active_power'], label='Power', lw=1)\n",
    "axes[0].plot(week.index, week['power_rolling_mean_24h'], label='24h Mean', lw=2, color='red')\n",
    "axes[0].set_title('Rolling Mean (24h)')\n",
    "axes[0].set_ylabel('kW')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Swish hour feature\n",
    "axes[1].scatter(df_hourly['hour'], df_hourly['hour_swish'], s=10, alpha=0.5)\n",
    "axes[1].set_title('Swish(Hour)')\n",
    "axes[1].set_xlabel('Hour')\n",
    "axes[1].set_ylabel('Swish')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Cyclical encoding\n",
    "axes[2].scatter(df_hourly['hour'], df_hourly['hour_sin'], s=10, alpha=0.5, label='sin')\n",
    "axes[2].scatter(df_hourly['hour'], df_hourly['hour_cos'], s=10, alpha=0.5, label='cos')\n",
    "axes[2].set_title('Cyclical Hour Encoding')\n",
    "axes[2].set_xlabel('Hour')\n",
    "axes[2].set_ylabel('Value')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16ac8a",
   "metadata": {},
   "source": [
    "## 4. Semi-Supervised Learning - Label Spreading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4bc7e9",
   "metadata": {},
   "source": [
    "Demonstrate semi-supervised learning using Label Spreading algorithm.\n",
    "This cell implements a minimal semi-supervised classification task where only 30% of samples have labels and the model must propagate labels to the remaining 70%. The target variable (power consumption) is discretized into 3 bins (Low/Medium/High) using quantile-based binning. LabelSpreading, a graph-based algorithm, constructs a k-nearest-neighbors graph and iteratively diffuses labels from labeled to unlabeled nodes using kernel functions. It leverages the manifold assumption that nearby points in feature space should share labels. Accuracy is measured on the originally unlabeled data to evaluate propagation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c558f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal Semi-Supervised Learning Demo\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "\n",
    "# Simple setup: use 30% labeled data to predict the rest\n",
    "data = df_hourly[['Global_active_power','hour','day_of_week','month']].dropna().iloc[:5000]\n",
    "X = data[['hour','day_of_week','month']].values\n",
    "\n",
    "# Convert continuous target to 3 bins (Low/Medium/High consumption)\n",
    "y = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile') \\\n",
    "        .fit_transform(data[['Global_active_power']]).ravel().astype(int)\n",
    "\n",
    "# Randomly hide 70% labels (simulate unlabeled data)\n",
    "n_labeled = int(len(X) * 0.3)\n",
    "rng = np.random.default_rng(42)\n",
    "labeled_idx = rng.choice(len(X), n_labeled, replace=False)\n",
    "y_semi = y.copy()\n",
    "y_semi[~np.isin(np.arange(len(X)), labeled_idx)] = -1  # -1 = unlabeled\n",
    "\n",
    "# Train LabelSpreading\n",
    "model = LabelSpreading(kernel='knn', n_neighbors=7, alpha=0.2, max_iter=30)\n",
    "model.fit(X, y_semi)\n",
    "\n",
    "# Evaluate on originally unlabeled data\n",
    "unlabeled_mask = (y_semi == -1)\n",
    "accuracy = (y[unlabeled_mask] == model.transduction_[unlabeled_mask]).mean()\n",
    "\n",
    "print(f\"Semi-Supervised Label Spreading\")\n",
    "print(f\"Total samples: {len(X)} | Labeled: {n_labeled} (30%) | Unlabeled: {unlabeled_mask.sum()} (70%)\")\n",
    "print(f\"Accuracy on unlabeled data: {accuracy*100:.1f}%\")\n",
    "print(\" Model successfully propagated labels to unlabeled samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730bccfb",
   "metadata": {},
   "source": [
    "## 5. Regression Models: Ridge and Gradient Boosting\n",
    "\n",
    "Now let's predict actual power consumption values using Ridge Regression and Gradient Boosting with our CUDA-engineered features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30fb898",
   "metadata": {},
   "source": [
    "### 5.1 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d47e65",
   "metadata": {},
   "source": [
    "Prepare feature matrix and target vector for regression modeling.\n",
    "This cell constructs the final modeling dataset by selecting 8 features (temporal and engineered) and the target variable (Global_active_power). It performs train-test splitting with 80/20 ratio using temporal ordering (shuffle=False) to respect time series structure. StandardScaler is applied to normalize features to zero mean and unit variance, preventing features with larger magnitudes from dominating gradient-based algorithms. The scaler is fit only on training data to avoid data leakage, then transforms both sets. Standardization improves Ridge regression convergence and ensures fair feature contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61081692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_cols = [\n",
    "    'hour', 'day_of_week', 'month', 'season',\n",
    "    'power_rolling_mean_24h', 'hour_swish', 'hour_sin', 'hour_cos'\n",
    "]\n",
    "\n",
    "df_clean = df_hourly[feature_cols + ['Global_active_power']].dropna()\n",
    "X = df_clean[feature_cols].values\n",
    "y = df_clean['Global_active_power'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nSamples: {len(X):,}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n",
    "print(\"StandardScaler applied (X_train_scaled / X_test_scaled ready)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2abe0",
   "metadata": {},
   "source": [
    "### 5.2 Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345b4ac",
   "metadata": {},
   "source": [
    "Train Ridge regression model with L2 regularization and evaluate performance.\n",
    "This cell implements Ridge regression, a linear model that minimizes squared error plus an L2 penalty term (alpha × sum of squared coefficients). The penalty prevents overfitting by shrinking coefficient magnitudes, especially useful when features are correlated. The model is trained on scaled features using alpha=1.0 regularization strength. Predictions are generated for both training and test sets, then evaluated using RMSE (root mean squared error, measuring average prediction error in kW) and R² (coefficient of determination, proportion of variance explained). Training time is recorded to benchmark computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "start_time = time.time()\n",
    "ridge = Ridge(alpha=1, random_state=42)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "ridge_time = time.time() - start_time\n",
    "\n",
    "y_pred_ridge_train = ridge.predict(X_train_scaled)\n",
    "y_pred_ridge_test = ridge.predict(X_test_scaled)\n",
    "\n",
    "ridge_train_rmse = mean_squared_error(y_train, y_pred_ridge_train) ** 0.5\n",
    "ridge_train_r2 = r2_score(y_train, y_pred_ridge_train)\n",
    "ridge_test_rmse = mean_squared_error(y_test, y_pred_ridge_test) ** 0.5\n",
    "ridge_test_r2 = r2_score(y_test, y_pred_ridge_test)\n",
    "\n",
    "print(f\"Ridge (scaled) Train RMSE {ridge_train_rmse:.4f} | R2 {ridge_train_r2:.4f}\")\n",
    "print(f\"Ridge (scaled) Test  RMSE {ridge_test_rmse:.4f} | R2 {ridge_test_r2:.4f} | Time {ridge_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378a0ca3",
   "metadata": {},
   "source": [
    "Train Gradient Boosting ensemble model for non-linear regression.\n",
    "This cell implements Gradient Boosting Regressor, an ensemble method that builds trees sequentially where each new tree corrects errors of the previous ensemble. It uses 200 weak learners (decision trees) with max_depth=6, learning_rate=0.1 to control step size in gradient descent. Unlike Ridge, Gradient Boosting captures non-linear relationships and feature interactions through recursive partitioning. The algorithm minimizes loss via gradient descent in function space, fitting residuals at each iteration. Performance is measured with RMSE and R² on both train/test sets, with timing recorded for comparison against Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dae508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "start_time = time.time()\n",
    "gb = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_time = time.time() - start_time\n",
    "\n",
    "y_pred_gb_train = gb.predict(X_train)\n",
    "y_pred_gb_test = gb.predict(X_test)\n",
    "\n",
    "gb_train_rmse = mean_squared_error(y_train, y_pred_gb_train) ** 0.5\n",
    "gb_train_r2 = r2_score(y_train, y_pred_gb_train)\n",
    "gb_test_rmse = mean_squared_error(y_test, y_pred_gb_test) ** 0.5\n",
    "gb_test_r2 = r2_score(y_test, y_pred_gb_test)\n",
    "\n",
    "print(f\"GB      Train RMSE {gb_train_rmse:.4f} | R2 {gb_train_r2:.4f}\")\n",
    "print(f\"GB      Test  RMSE {gb_test_rmse:.4f} | R2 {gb_test_r2:.4f} | Time {gb_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b4367",
   "metadata": {},
   "source": [
    "### 5.4 Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cf101",
   "metadata": {},
   "source": [
    "Compare model performance using RMSE-focused visualization and metrics table.\n",
    "This cell generates a comprehensive comparison of Ridge and Gradient Boosting models, displaying a tabular summary of train/test RMSE, R², and training time. It identifies the best model by minimum test RMSE since RMSE directly measures prediction error in kilowatts, the interpretable unit for energy forecasting. A 2×2 scatter plot grid visualizes predicted vs. actual values for both models on train and test sets, with perfect prediction shown as red diagonal lines. Deviations from this line reveal model bias and variance. The visualization enables qualitative assessment of prediction quality and identification of systematic errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc05ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison - Focus on RMSE\n",
    "print(\"\\n MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Ridge', 'Gradient Boosting'],\n",
    "    'Train RMSE': [ridge_train_rmse, gb_train_rmse],\n",
    "    'Test RMSE': [ridge_test_rmse, gb_test_rmse],\n",
    "    'Test R²': [ridge_test_r2, gb_test_r2],\n",
    "    'Time (s)': [ridge_time, gb_time]\n",
    "})\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "best_model = 'Ridge' if ridge_test_rmse < gb_test_rmse else 'Gradient Boosting'\n",
    "print(f\"\\n Best: {best_model}\")\n",
    "\n",
    "# Visualization - Prediction vs Actual (RMSE focus)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "axes[0, 0].scatter(y_train, y_pred_ridge_train, alpha=0.3, s=5)\n",
    "axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "                'r--', linewidth=2)\n",
    "axes[0, 0].set_title(f'Ridge - Train (RMSE={ridge_train_rmse:.4f} kW)', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Actual')\n",
    "axes[0, 0].set_ylabel('Predicted')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].scatter(y_test, y_pred_ridge_test, alpha=0.3, s=5, color='orange')\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2)\n",
    "axes[0, 1].set_title(f'Ridge - Test (RMSE={ridge_test_rmse:.4f} kW)', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Actual')\n",
    "axes[0, 1].set_ylabel('Predicted')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].scatter(y_train, y_pred_gb_train, alpha=0.3, s=5, color='green')\n",
    "axes[1, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "                'r--', linewidth=2)\n",
    "axes[1, 0].set_title(f'GB - Train (RMSE={gb_train_rmse:.4f} kW)', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Actual')\n",
    "axes[1, 0].set_ylabel('Predicted')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].scatter(y_test, y_pred_gb_test, alpha=0.3, s=5, color='purple')\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2)\n",
    "axes[1, 1].set_title(f'GB - Test (RMSE={gb_test_rmse:.4f} kW)', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Actual')\n",
    "axes[1, 1].set_ylabel('Predicted')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n CUDA features used in both models\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e14cac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c06f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
